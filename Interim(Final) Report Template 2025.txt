A Study on Robust Recommender System using Disentangled Contrastive Collaborative Filtering

An interim report submitted in partial fulfilment of the
requirements for the award of the degree of Master of Software Engineering and Data Science 



















 A Study on Robust Recommender System using
 Disentangled Contrastive Collaborative Filtering  

  Abstract

Recommender systems (RS) are the most usable systems on digital platforms. People use RS to find useful solutions and make better choices by getting personal suggestions. Since these systems help people decide, they must be strong. If the data is wrong, unfair, or changed on purpose, it still needs to give correct suggestions. collaborative filtering, content-based systems, and hybrid models have been used previously to make them better.  However, they have problems when the data is noisy or under attack.

Recent progress in contrastive learning (CL) has introduced self-supervised techniques that improve robustness by learning more stable and general representations from noisy or limited data. Self-Supervised Graph Learning (SGL), Simplified Graph Contrastive Learning (SimGCL), and Disentangled Graph Collaborative Filtering (DGCF) are the model that used to improve performance. They still face problems like noisy data and mixed user interests. 

To solve these issues, Disentangled Contrastive Collaborative Filtering (DCCF) uses smart data augmentation to handle noisy data. It also separates user interests to better understand different preferences. Additionally, it uses cross-view contrastive learning to make the model more accurate and better at handling natural noise. However, DCCF still has some limitations: it lacks protection against adversarial attacks, has high computational costs, assumes fixed noise patterns, and faces instability during early training. This review focuses on these gaps for future research to build adaptive, efficient, and robust recommender systems that can handle dynamic, large-scale, and adversarial environments effectively. 
 
  KEYWORDS
Recommender Systems
Contrastive Learning
Robustness
Noise Handling
I. INTRODUCTION  

People spend a lot of time using different online services and platforms in today's digital world. Recommender systems (RS) are now a big part of everyday life. They help you decide what to watch on Netflix. They also suggest what to buy on Amazon. They can even guide you on health-related decisions. These systems make decision-making easier by sorting data. They remove unhelpful information from large amounts of data to save time. They also provide personal recommendations based on each person’s preferences and needs. According to Hu et al. (2024), recommender systems (RS) are important because they improve user experiences by showing the most relevant and useful products based on each user’s behavior and interests. 

As RS become a bigger part of today’s daily life, it is more important to make sure they are strong, accurate, and reliable. Robustness is the system’s feature to provide accurate and correct suggestion even though data is noisy and ensuring high-quality results (Ma et al. 2023). Recommender systems are crucial for user trust, satisfaction, and financial losses (Zhang et al. 2023). Over time, they have evolved from content-based methods and collaborative filtering to advanced hybrid frameworks and graph neural network-based models, improving accuracy, personalization, and scalability.

However, as models have become more advanced and complex, they have also become easier to break and less stable.
 Even small data errors or malicious changes can seriously affect their performance. Zugner et al. (2020) highlighted that advanced recommender system models can be easily affected by small amounts of noise or intentional attacks, showing the need for more reliable and robust solutions. 
 
CL has recently surfaced as a viable strategy for enhancing recommender system resilience in order to address these vulnerabilities. By separating meaningful signals from misleading ones, CL helps models learn better representations and become less susceptible to data noise and inconsistencies. It compares and aligns similar data points while separating dissimilar ones using self-supervised techniques, resulting in more stable and broadly applicable embeddings.  
This literature review examines the robustness of recommender systems, focusing on contrastive learning-based approaches. It discusses several modern methods, including Self-Supervised Graph Learning (SGL), Simplified Graph Contrastive Learning (SimGCL), and Disentangled Graph Collaborative Filtering (DGCF). The main focus will be on Disentangled Contrastive Collaborative Filtering (DCCF). Among these, DCCF represents a significant step forward by combining adaptive data augmentation, disentangled intent modelling, and cross-view contrastive learning to achieve better resilience against natural noise. However, DCCF still has a number of drawbacks in spite of its advancements. The effectiveness of this system in large-scale, dynamic environments is limited by its reliance on static noise assumptions, instability in early training, high computational costs, and lack of adversarial defenses.  
 
This review offers a more thorough understanding of the state of robust recommender systems today by critically examining the advantages and disadvantages of these strategies. In order to create adaptive, scalable, and genuinely robust recommender systems that can function dependably in noisy, dynamic, and real-world settings, it also identifies research gaps and establishes the groundwork for future advancements.

Aim of the Research 
The aim of this project is to evaluate and improve the robustness of contrastive learning-based recommender systems when dealing with natural noise conditions. The study specifically investigates how well the Disentangled Contrastive Collaborative Filtering (DCCF) model performs when subjected to both static noise (fixed corruption levels) and dynamic noise (patterns like ramp-up, burst, and shift). Additionally, it explores the use of a self-paced warm-up strategy to address early-training instability and enhance the model’s overall stability and accuracy in noisy environments.

Project Scope
* The study centers on contrastive learning-based recommender systems, with DCCF as the primary model of investigation.
* The project evaluates the model’s performance under two types of noise.
Static noise: fixed corruption rates in the data.
Dynamic noise: changing noise patterns over time, including ramp-up, burst, and shift.
* It applies contrastive learning techniques, using Disentangled Contrastive Collaborative Filtering (DCCF) as the baseline model to analyze recommendation performance.
* Noise injection strategies are used to create more realistic noisy environments, helping test the model under conditions that closely resemble real-world data challenges.
* A self-paced warm-up strategy is applied to minimize instability during the early stages of training and enhance the model’s overall stability and accuracy when working with noisy data.
* Benchmark datasets such as Gowalla, Amazon-Book, and optionally MovieLens-1M.
The study does not cover adversarial attacks, large-scale computational optimization, deployment pipelines, interpretability, or model explainability.

Structure of the Interim Report 
* Introduction: Overview of recommender systems, noisy data issues, and research purpose. 
* Literature Review: Existing methods and research gaps. 
* Methodology: Model, noise simulation, and warm-up approach. 
* Results and Analysis: Experimental results and performance comparison. 
* Conclusion and Future Work: Key findings and future directions.




II. LITERERATURE REVIEW
Introduction of literature review

In today's digital landscape, individuals tend to spend a significant portion of their time engaging with online services, where recommender systems (RS) play a pivotal role in helping users navigate through the overwhelming content. Whether it is for clinical decision-making in healthcare, Netflix streaming, or Amazon product recommendations, RS have become integral across industries. According to Hu, Li, Cui, and Yi (2024), RS assist users by filtering out irrelevant information and identifying the most relevant and useful aspects of a product or service.
However, as RS becomes more embedded in everyday decision-making, heir reliability becomes paramount. One critical measure of such performance is known as robustness, which refers to the degree to which a system can perform consistently when the data it is trained or tested on is exposed to noise, bias, or even adversarial attacks. To reinforce this, Ma et al. (2023) describe robustness as the capacity to withstand perturbations and still deliver accurate recommendations. Without it, RS are vulnerable to performance degradation, which can result in poor suggestions, dissatisfied users, and potential business losses (Zhang et al. 2023).
Understanding the importance of robustness requires examining the evolution of RS. Two foundational approaches in early RS development were collaborative filtering and content-based methods (Shvarts et al. 2017, as cited in Sinha & Dhanalakshmi, 2019). These were relatively simple but limited in scope. Advances in deep learning and graph neural networks (GNNs) have pushed the boundaries of accuracy and personalization (Hu et al. 2023). However, as Zugner et al. (2020) highlight, these models are also more fragile and can be easily thrown off by even minor changes in data or focused attacks. To address these vulnerabilities, contrastive learning (CL) has emerged as a promising approach for building robust representations by teaching models to better distinguish between meaningful and misleading signals.
This research focuses on robustness in RS, specifically by looking at how systems would respond to natural noise, adversarial attacks, and bias. It examines key CL-based methods, including Self-supervised Graph Learning (SGL), Simplified Graph Contrastive Learning (SimGCL), Disentangled Graph Collaborative Filtering (DGCF), and the anchor model Disentangled Contrastive Collaborative Filtering (DCCF). Topics such as deployment pipelines, interpretability, and robustness in non-RS domains, such as computer vision (CV) and natural language processing (NLP) are excluded from scope.
Robustness is not merely a technical concern but central to user trust, fairness, and scalability. While DCCF represents a strong attempt to address robustness, especially in natural noise, it does not fully address other robustness challenges. This review critically evaluates DCCF’s contributions, compares it with alternative approaches, and identifies gaps that future research could address.
To guide the discussion, the review is organized into six thematic sections: (1) RS background, (2) robustness challenges, (3) contrastive learning approaches, (4) the DCCF model, (5) its limitations, and (6) the research gap leading to a proposed direction.

Body of literature review
2.1 Recommender System Background 
Over time, recommender systems have undergone substantial transformation, moving from simple heuristic-based techniques to complex neural architectures. This section explores that progression in greater depth, emphasizing the changes made in key approaches and the motivations behind them. Beginning with early collaborative filtering techniques and advancing toward graph-based models, the research highlights how each stage has contributed to capturing increasingly complex and multiform user-item relationships.
 
One of the earliest and most widely adopted approaches is collaborative filtering (CF), particularly in e-commerce platforms. According to Hamidi and Moradi (2024), CF contributes to users' ability to make better decisions while simultaneously increasing sales for companies. It operates by analyzing user-item interactions, usually ratings, to identify patterns of similarity. It builds user-user or item-item similarity matrices and uses neighborhood-based algorithms to generate recommendations (Bag et al. 2019).
 
Two prominent CF techniques are k-nearest neighbors (kNN) and matrix factorization (MF). kNN identifies clusters of similar users based on shared ratings and predicts preferences using the average ratings of top-k neighbors. In contrast, MF reduces the user-item matrix into latent factors, allowing it to capture hidden relationships and provide predictions that can be used on a larger scale. By integrating implicit feedback and temporal dynamics, MF has often outperformed kNN, which is reinforced by a study by Dong et al. (2022). However, even with all its advantages, CF still has drawbacks, including cold-start issues and sparsity, particularly in environments with limited user-item interactions.
 
To address these gaps, content-based filtering (CB) was introduced. It matches items to user preferences by making recommendations based on attributes such as genre, keywords, or descriptions (Maulana & Setiawan 2024). Despite its strengths, both CF and CB do have their limitations, which have led to the development of hybrid systems. Thes model integrates CF and CB to improve accuracy and coverage. Kumar and Bhasker (2020) describe hybrid RS as using embeddings to represent users and items, facilitating the learning of non-linear latent factors. Hybrid systems also help mitigate cold-start problems by incorporating side information into deep neural networks and are widely used on platforms like Amazon and Netflix.
 
More recently, graph neural networks (GNNs) have redefined RS by modeling them as bipartite graphs, allowing for deeper relational learning through message passing (Wang et al. 2019). These models were able to take into account multi-hop connections and have led to influential architectures like NGCF and LightGCN, offering enhanced personalization and scalability.
 
While accuracy has always been an essential priority, robustness has emerged as a critical concern. As per Ma et al. (2023), robust RS must remain stable under noisy or adversarial conditions, for a deeper exploration of resilience in the next section.

2.2 Robustness Challenges
Recommender systems (RS) suggest products, content, and services based on what users like, but their accuracy decreases when the data is incomplete, incorrect, or changed (Ray & Mahanti, 2010). Robustness means the system can still give accurate and reliable suggestions even when the data is noisy or imperfect.A robust system can handle missing data, user changes, mistakes, or fake ratings and still give good and trustworthy recommendations (Zhang, 2023). Researchers work to improve the robustness of recommender systems to make them more accurate and reliable.This helps RS maintain good performance even when the data is noisy, incomplete, or manipulated. 
According to Guarrasi et al (2024), a robust RS can provide unbiased and right choices with missing data, biased patterns, or deliberate manipulation issues. If user behavior changes or the environment is different, the system should still work well and give correct results. Researchers have suggested different ways to solve these problems to make systems stronger. For natural noise, they use methods like finding errors, filtering bad data, and fixing wrong predictions. 
In response to adversarial attacks, approaches like adversarial training, regularization, and graph-based anomaly detection are utilized to recognize fake profiles and manipulative patterns. Fairness-aware recommender systems change ranking algorithms to reduce bias. They also adjust loss functions to make recommendations more balanced.

Additionally, they re-rank results to provide fair exposure and more diverse suggestions. Most existing solutions focus on solving one robustness problem at a time. New techniques like contrastive learning and robust representation aim to handle multiple issues together. However, making systems both strong and scalable is still hard. Many methods need a lot of computing power, so they are not good for large or real-time systems. Moreover, when user behavior and data patterns change, fixed solutions often stop working well. Achieving truly robust, flexible, and efficient recommender systems remains an open research challenge.
2.3 Contrastive Learning for RS 
In scenarios where labelled data is sparse or unavailable, Contrastive Learning (CL) has emerged as a promising self-supervised approach for improving robustness in recommender systems (RS). Researchers aim to enhance robustness so that RS can maintain performance even under noisy, incomplete, or manipulated data conditions
At its core, contrastive learning trains models by comparing pairs of data. Positive pairs refer to similar inputs, such as different views of the same user or item. In contrast, negative pairs represent dissimilar inputs, typically drawn from different anchor points or distinct user-item subgraphs. The model is able to learn stable and generalizable representations by aligning positive pairs and separating negative ones in the latent space (Wang & Isola 2020).
Graph contrastive learning, in particular, has shown strong performance without relying on labels. As Liu et al. (2025) explained, this learning is considered as a common self-supervised paradigm that contrasts augmented views of user-item graphs to improve embedding quality. This is accomplished by creating negative pairs from different anchors and positive pairs from the same anchor graph. By minimizing the distance between similar data points, this alignment enhances representation consistency. 
In RS, CL is often applied to graph-structured data. Yu et al. (2022) highlight that CL has resurged in deep representation learning due to its ability to extract general features from large volumes of unlabeled data. It also acts as a natural remedy for data sparsity and overfitting. A commonly used method is to build up structural perturbations, like node dropout or stochastic edges, to the user-item bipartite graph. A graph encoder is then used to maximize representation consistency across views. In this case, the CL task serves as an auxiliary objective that is jointly optimized with the primary recommendation task.
Addressing robustness in RS is never-ending; in fact, several CL-based models have been proposed to operationalize these ideas in RS. In order to produce contrastive views, Zhang et al. (2023) mentioned self-supervised graph learning (SGL), which makes use of graph augmentations such as node dropping, edge masking, and random walk sampling. These perturbations will help the model generalize better in sparse environments by reducing sensitivity to noise.
Building on these augmentation strategies, Simple Graph Contrastive Learning (SimGCL) simplifies this process by injecting random noise directly into the embedding space during training, which allows the avoidance of complex structural augmentations. Although this increases its efficiency, it might also make it more challenging for the model to identify subtle patterns in user behavior.
In contrast to noise-based methods, Disentangled Graph Collaborative Filtering (DGCF) takes a different approach by separating user intents. Wang et al. (2020) argue that treating all interactions equally overlooks the diverse reasons users engage with items. DGCF splits embeddings into multiple components, with each being tied to a specific intent, and subsequently uses a neighbor routing mechanism to refine these representations. To ensure that the intents remain independent, a distance-correlation regularizer will help improve interpretability and personalization.
Overall, it is clear that CL has several advantages: improved robustness, generalizing well in noisy or sparse data, and effective scaling in self-supervised settings (Yu et al. 2022). However, augmentations may introduce irrelevant noise, and overlapping user intents can destabilize disentangled representations, which remain unresolved challenges in current CL-based models. These limitations imply that, despite CL's powerful nature, its design must be meticulously tuned to prevent unforeseen effects.

2.4 Disentangled Contrastive Collaborative Filtering (DCCF)
Contrastive learning-based recommender systems have made impressive advances in the last few years, but still the performance is fragile due to two essential challenges: augmentation noise and entangled intent representations. Augmentation noise occurs with artificially generated views of a user-item interaction graph that contain perturbations that are not indicative of meaningful user behaviors. Random dropout of edges or nodes, a common technique in contrastive approaches (including SGL and SimGCL), can unintentionally bias true preference signals or, more generally, remove important interactions, undermining the quality of learned embeddings. Another weakness is entangled intents, in which several user interests, say preference among genres or brands or categories, are summarized in a single vector. This complicates the differentiation and capture of the unique dimensions of user behavior into the model and lowers the strength and interpretability of recommendations.
To address these problems, Ren et al. (2023) have presented the framework of the newly developed Disentangled Contrastive Collaborative Filtering (DCCF) that integrates disentanglement learning with adaptive contrastive augmentation. DCCF is driven in both directions: to reduce the negative effect of noisy graph augmentations by introducing adaptive components, and to encode multiple user motivations in the explicit form of disentangled latent factors. It is a combination of these two dimensions that caused DCCF to build a more robust and generalizable recommender-system capable of processing noisy real-world data better than the earlier contrastive learning approaches.
DCCF presents three fundamental contributions. The intent prototype mechanism is the first, in which a learning mechanism acquires a set of prototype vectors to describe various latent intents. The model does not try to compact all user preferences in a single embedding, but rather separates it into a series of dimensions, allowing user-item interactions to be captured at very fine-grained scales. This method is inspired by the clustering-based representation learning, which uses prototypes as anchors of various behavior semantic features. The second input is adaptive augmentation through learnable masks. DCCF uses a parameterized mask generator (versus static random perturbations) to perturb the edges in the user-item graph selectively. This guarantees that augmentations are not arbitrarily noisy but rather guided by the data distribution behind them, further enhancing the accuracy of contrastive signals. The third one is cross-view contrastive learning. This consistency between global and local user intent representations is enforced by aligning embeddings across several disentangled views, which the model carries out. This multi-view contrastive task prevents overfitting and improves learned embedding stability.
The empirical analysis of DCCF shows that it can be effective in a variety of benchmark datasets such as Gowalla, Tmall, and Amazon-book. DCCF scores higher on both Recall and NDCG metrics than the more than ten competing models, including LightGCN, SimGCL, DGCF, and SGL. The gains are especially pronounced in noisy data situations, in which random augmentations or entangled embeddings would tend to deteriorate performance. These findings support the claim that de-entangling user intents and using adaptive augmentation create stronger and more accurate recommendations. Notably, DCCF exhibits similar improvements compared to DGCF which is another disentanglement-based approach by showing that contrastive signals have a complementary effect on stabilizing intent separation.
These are good results, but there are a few limitations. First, the strength of DCCF is mainly tested in the presence of natural noise, i.e., accidental clicks or incorrectly labelled ratings, yet it lacks defenses against adversarial examples. DCCF is not adversarially trained and has no detection mechanism, thus might not work in hostile settings. Second, adaptive augmentation is better than random dropout, but it also results in more computation. Training and using parameterized masks consume more additional memory and training time than lightweight contrastive models such as SimGCL, potentially restricting the ability of large-scale systems to scale. Third, the approach assumes that patterns of noise are relatively fixed during training. But, the patterns of user activities and noise vary over time, especially in dynamic systems like social media or e-commerce when it comes to seasonal campaigns. Mechanisms of temporal adaptation are absent in DCCF, so this approach is limited in its application in rapidly evolving situations. Lastly, the use of disentangled prototypes may introduce instability at an early stage. In early training, the prototypes might not yet be able to capture meaningful intents, and the mask generator will drop informative interactions or keep noisy ones. Unless handled, this instability may slow convergence and influence performance.
Concisely, DCCF represents a significant advancement in contrastive recommender systems to construct a powerful recommender system through noise reduction in augmentation and disentanglement of user intents, adaptive augmentation, and cross-view contrastive learning. Nevertheless, its weaknesses, such as the absence of adversarial defenses, computation complexity, the assumption of fixed noises, and early training, suggest that future studies are needed to develop recommender systems that are not only resilient to natural noise, but also adaptive, computationally efficient, and robust against adversarial manipulation.
2.5 Limitations & Gaps
While Disentangled Contrastive Collaborative Filtering (DCCF) shows a substantial advancement over prior contrastive recommender models, its design and evaluation reveal several limitations that point to important gaps in the research landscape. These gaps provide opportunities for developing the next upgrade of robust recommender systems that can cope with more complex, large-scale, and evolving environments. In this study we target limitations (3) and (4) under natural (non-adversarial) noise. We keep DCCF’s architecture unchanged and add two training-time fixes—(i) a static confidence denoiser that down-weights likely noisy/over-exposed interactions (Toledo, Mota & Martínez 2015) and (ii) an exposure-aware distributionally robust objective (DRO) applied after a short burn-in/warm-up (Yang et al. 2024).

2.5.1 Lack of adversarial defense.
DCCF is explicitly designed to address natural noise such as random misclicks or mislabeled ratings. However, it does not include any adversarial training or detection mechanisms. In practice, recommender systems are often targeted by malicious perturbations such as shilling attacks, fake users, and coordinated manipulation campaigns. Prior studies have demonstrated that such adversarial inputs can significantly distort ranking outcomes if the model has not been hardened (He et al. 2018; Yuan et al. 2019; Zhang et al. 2023). The absence of adversarial resilience in DCCF limits its applicability in open platforms such as e-commerce or social media where hostile behaviors are common.

2.5.2 Computational overhead
Compared to lightweight models such as LightGCN (He et al. 2020) or SimGCL (Yu et al. 2022), DCCF introduces additional complexity through the use of multiple intent prototypes and learnable graph masks. These components provide richer representations and more accurate augmentations, but they also require greater memory and training time. While DCCF is more efficient than earlier disentanglement methods such as DGCF (Wang et al. 2020), it still falls behind the scalability of simpler contrastive frameworks. For large-scale or real-time recommender environments, this overhead may restrict deployment. Thus, there is an open gap for research into models that balance disentangled robustness with computational efficiency.

2.5.3 Assumption of static noise
Another critical limitation lies in DCCF’s underlying assumption that noise patterns remain stable throughout training. Its evaluation considers noise as a static property of the dataset. Yet in real-world systems, noise evolves over time: seasonal shopping campaigns, sudden shifts in popularity, or temporary bursts of user inattention can all change the distribution of noisy interactions. Temporal recommendation research (Kang & McAuley 2018) and noise management studies (Baldán et al. 2024) highlight that static treatments are insufficient. Without temporal adaptation, DCCF risks degraded robustness in dynamic user environments. This limitation is particularly relevant to our project, which focuses on simulating dynamic noise injection to evaluate how DCCF behaves when noise varies over training epochs. In our work these dynamic patterns are treated as evaluation stress tests (ramp-up, burst, shift) to emulate drift (Baldán, Yera & Martínez 2024), while the training fix is loss re-weighting rather than architectural changes.

2.5.4 Early-training instability
Finally, DCCF’s reliance on prototypes and adaptive masks introduces a risk of instability during early training. In the initial epochs, prototypes may not yet represent meaningful intents, and the mask generator may discard valuable edges or retain noisy ones. Such behavior can slow convergence and lead to unstable learning curves. Related work in curriculum learning and self-paced training (Xie et al. 2020) has shown that gradually increasing the strength of denoising after a warm-up phase can improve stability. Yet DCCF does not include such mechanisms, leaving a gap for future exploration of training schedules that reduce early noise amplification. Accordingly, we adopt a brief burn-in (warm-up)—training in an easier regime for W epochs—before enabling noise schedules and DRO, a staged strategy shown to stabilize deep models (Xie et al. 2020).

Taken together, these four limitations highlight the boundaries of DCCF’s contribution. It improves robustness against natural noise but does not address the four stated limitations. The research gap therefore lies in combining DCCF’s strengths with additional robustness: adversarial resistance, computational scalability, temporal adaptation, and stability under early noise. In this report we specifically address temporal adaptation and early stability for natural noise by (a) evaluating DCCF under static and dynamic noise stress tests and (b) introducing training-time re-weighting—a static confidence denoiser (Toledo 2015) and an exposure-aware DRO objective applied after warm-up (Yang et al. 2024).

Conclusions from the literature review
The literature review has traced the progression of recommender systems from early collaborative filtering and content-based methods to hybrid frameworks and, more recently, graph neural networks. Each stage has expanded the representational capacity of recommender models, enabling more accurate and scalable predictions. However, as systems have grown more complex, the issue of robustness, which is their ability to perform reliably under noisy, biased, or adversarial data, has become increasingly essential.

Several approaches have been developed to enhance robustness. Methods that address natural noise focus on detecting and filtering unreliable interactions, while those that target adversarial attacks employ adversarial training or graph-based defenses. Fairness-aware recommender systems mitigate systematic biases. More recently, contrastive learning has emerged as a promising paradigm, leveraging self-supervised signals to improve representation learning in sparse or noisy environments. Models such as SGL, SimGCL, and DGCF have shown that augmentations and disentanglement can strengthen resilience, but each still faces significant trade-offs.

Within this trajectory, DCCF represents an important step forward. By combining disentangled user intent prototypes with adaptive graph augmentation and cross-view contrastive learning, it achieves state-of-the-art performance on multiple benchmarks. DCCF demonstrates clear improvements in handling natural noise, outperforming earlier contrastive and disentangled approaches. In this study we keep the DCCF architecture unchanged and act only at training time. Specifically, we use (i) a static confidence denoiser that down-weights likely noisy/over-exposed interactions and (ii) an exposure-aware, DRO-style re-weighting that is enabled after a short burn-in (warm-up) to avoid early-epoch instability

Nevertheless, four major limitations remain. DCCF lacks adversarial defense, introduces additional computational overhead, assumes static noise distributions, and suffers from early-training instability. These gaps limit its deployment in dynamic, large-scale, or hostile environments. Among these, the assumption of static noise and early instability are especially relevant to this project, as they reflect challenges in realistic user scenarios where behaviors and error patterns evolve over time.

In conclusion, the literature review highlights both the expectations and shortcomings of current robustness research in recommender systems. The practical gap we pursue is natural-noise robustness with temporal drift and early-epoch stability, without modifying DCCF’s modules. Accordingly, we will: (a) evaluate DCCF under static and dynamic train-only noise stress tests (validation/test kept clean), and (b) introduce two loss-side fixes - static confidence denoising (always on) and exposure-aware DRO (on after burn-in) - to reduce robustness drop while preserving the original model. Results are reported with Recall@20, NDCG@20, Robustness Drop (%), and time/epoch.
III. APPROACH
  
Describe the approach you have taken to conduct your research or development, including the methods, techniques, or frameworks applied.
Clearly explain the problem formulation or client requirements, if applicable, that guided your work.
Outline any assumptions made and the limitations of your work (e.g., constraints in scope, methodology, resources, or implementation).

Problem definition.
 This study investigates robust recommendation under natural (non-adversarial) noise, where user–item logs contain spurious positives (e.g., misclicks) and exposure/popularity effects. The backbone model is Disentangled Contrastive Collaborative Filtering (DCCF), chosen for its ability to reduce augmentation noise and disentangle user intents without side information (Ren et al. 2023). For context and comparison, we consider LightGCN (He et al. 2020) and SimGCL (Yu et al. 2022).
Design principles.
Evaluation under realistic noise regimes. We examine both static corruption (fixed noise rate on the training split) and dynamic corruption (time-varying schedules such as ramp-up, burst and shift) to emulate temporal drift observed in practice (Baldán, Yera & Martínez 2024). Static corruption uses simple flip/add operators—removing true positives or injecting false positives—grounded in noisy-rating correction for CF (Toledo, Mota & Martínez 2015). Validation and test sets are kept clean to avoid confounding measurement.
Training-time robustness without changing architecture. Rather than modifying DCCF’s modules, we apply loss re-weighting at training time:
a static confidence denoiser cu,i  that down-weights likely noisy/over-exposed interactions using an item-popularity proxy (Toledo, Mota & Martínez 2015);
an exposure-aware distributionally robust objective (DRO) that, after a short burn-in (warm-up), emphasizes the hardest examples (top-q% per-sample losses; CVaR-style) while adding a mild penalty for high exposure to curb bandwagon effects (Yang et al. 2024).
Stabilisation via a brief burn-in. For the first WWW epochs (default W=10), training proceeds in an easier regime (no injected noise; gentler contrastive temperature). After burn-in, noise schedules and DRO weighting are enabled. This staged “easy?hard” scheduling is widely used to improve stability in modern deep recommenders (e.g., curriculum/self-paced training) and pairs naturally with DCCF’s contrastive objectives (Zhu et al. 2021; Xie et al. 2020).

Research questions:
* RQ1: How does DCCF’s top-K accuracy change under static versus dynamic natural noise?
* RQ2: Does a burn-in phase improve early-epoch stability under noise?
* RQ3: Does exposure-aware DRO reduce robustness drop relative to vanilla DCCF (with burn-in)?

Tooling and metrics.
 All models are trained/evaluated in RecBole for standardized data loading, training loops and top-K metrics (Zhao et al. 2020). We report Recall@20 and NDCG@20 as primary ranking metrics (Cremonesi, Koren & Turrin 2010) and derive Robustness Drop (%) relative to the clean baseline to quantify sensitivity to noise.

Assumptions & scope.
 The focus is natural noise; adversarial attacks, deployment engineering and model explainability are out of scope. Datasets follow common RS practice (e.g., Gowalla, Amazon-Book); popularity is estimated from the training split only. Dynamic schedules are stress tests for robustness, not a learning method (Ren et al. 2023; He et al. 2020; Yu et al. 2022; Baldán et al. 2024; Toledo, Mota & Martínez 2015).



IV. EXECUTION

In this section, describe how you have carried out your research or project. Explain how data was gathered, how your approach or methodology was implemented, and how any software, system, or prototype was developed. If applicable, outline how project requirements were managed to achieve the stated aims or deliver the intended product.


Framework and environment.
 Implementation uses RecBole (PyTorch) to host DCCF as a GeneralRecommender producing per-sample recommendation and contrastive losses (Zhao et al. 2020; Ren et al. 2023). Experiments are tracked (e.g., CSV/Epoxy export) to generate tables summarising metrics per run/seed.
Data and splits.
 Gowalla and Amazon-Book are prepared with standard train/validation/test splits consistent with prior work (He et al. 2020; Yu et al. 2022). Item popularity is computed on the training split and used both for the static confidence cu,i    and as an exposure proxy in DRO. Validation/test remain noise-free.
Noise generation (train-only).
* Static noise: apply flip/add to positives at fixed rates p?{0.05,0.10,0.15,0.20} (Toledo et al. 2015).
* Dynamic noise (stress tests):
o Ramp-up: pt  increases linearly with epoch;
o Burst: spike pt  for a short window;
o Shift: switch corruption focus (e.g., long-tail ? head).
 These patterns emulate non-stationary noise and seasonality (Baldán et al. 2024).
Training schedule and objectives.
1. Burn-in 1..W epochs: no injected noise; gentler contrastive temperature; DRO disabled.
2. Post burn-in: enable the chosen noise schedule on the training split; compute per-sample losses and apply two weights at loss time:
a. Static confidence cu,i=max?(cmin?,?1???pop(i)) (Toledo et al. 2015);
b. DRO weight wu,i  that combines CVaR focus on the top-q% hardest samples with a small exposure penalty (1??pop(i)) (Yang et al. 2024).
 The batch objective is the normalized weighted sum of recommendation and contrastive losses.
3. Ablations and seeds: runs include Clean ? Static10 ? Dynamic-Burst ? +Burn-in ? +DRO; results are averaged over multiple seeds (mean ± sd).
Evaluation and reporting.
 Per-epoch Recall@20/NDCG@20 are recorded on validation; final results include test-set metrics, Robustness Drop (%) versus the clean baseline, and time/epoch for efficiency (Cremonesi, Koren & Turrin 2010). Tables and plots are produced from the exported CSVs.


  
V. ANALYSIS AND DISCUSSION OF RESULTS
  
4.1 Verification of Baseline Performance
The initial work in the analysis was to ensure that the PyTorch code of DCCF recreates the baseline scores found in Ren et al. (2023). Recall at 20 and NDCG at 20 reached similar values on the clean Gowalla data as found in the original paper, indicating both the accuracy of the implementation itself and the validity of the experiment itself. Such validation is crucial, as it creates a valid reference point: other experiments on noisy data sets can be usefully compared to the clean baseline without worrying about implementation errors.

4.2 Impact of Static Noise
To test robustness against fixed noise distributions, the training data was corrupted by fixed 5, 10, 15 and 20 percent noise. In line with expectation, the performance deteriorated with increased level of corrupted interactions. In one instance, Recall20 at 10% static noise indicated a significant decrease compared with the clean baseline (need to verify values in the final report). This is in line with previous studies (Toledo et al. 2015; Baldán et al. 2024), which highlight how any minor amount of noise during user-item interaction may skew such collaborative cues. 

Significance: These results show that even though adaptive augmentation of DCCF makes it more resilient than its predecessors, such as SimGCL, it does not degrade. This proves that the mere existence of immobile natural noise is a cost to performance that can be quantified and underscores the importance of robustness as a performance assessment tool.

4.3 Impact of Dynamic Noise

The second experiments added the dynamic noise distributions that changed on training epochs. Three patterns were tested: 
Ramp-up: the noise is slowly increasing through epochs. 
Burst: abrupt bursts of loud noise at certain times. 
Shift: corruption type flips during the training (e.g. tail-item corruption becomes head-item corruption). 
 
In all the three dynamic environments, DCCF was more unstable than static noise. Recall@20 dropped at a steeper rate in ramp-up schedules than in the non-varying 10 case, which suggests that the masks and prototypes used by the model were unable to adjust the changing circumstances. Burst settings caused short-term performance collapse, which represented instability when there was an abrupt change in distribution. Of special concern was the shift pattern, which indicated that DCCF is also problematic when the character of noise varies with time. 
 
Significance. The findings of these studies underline one of the key limitations of DCCF that the model is stipulated to be robust under the assumption of noise being constant and generalisability to non-stationary or changing data does not exist. As a real-world user behavior usually varies with seasonal campaigns, surges in popularity, or alterations in consumption patterns, this result suggests a severe weakness in existing robust recommender systems.

4.4 Effect of Warm-Up Strategy

In order to stabilize the early-training, the self-paced warm-up methodology was experimented, in which the noise injection was postponed during the initial W epochs (e.g. W = 10). The warm-up in the static noise condition and the dynamic noise condition was always superior to the noisy training. The greatest returns were in the first epochs, when instability was minimized by means of warm-up training and disentangled prototypes generated more powerful representations before noise was introduced. 
 
Justification. Such provisional results are in line with the body of literature on self-paced learning so far (Xie et al., 2020), which hypothesizes that a progressive increase in the level of difficulty enhances convergence in noisy environments. 
 
Significance. The warm-up strategy is a fairly straightforward, but powerful, way to address one of the largest constraints of DCCF early-training instability. This is not a full answer to dynamic noise adaptation but shows that relatively lightweight changes can be made to help to increase stability. 

4.5 Overall Discussion

Taken together, the results provide an inconsistent picture of the power of DCCF. On the one hand, it is effective in clean and moderately noisy non-stirred conditions, which explains adaptive augmentation and intent disentanglement. On the other hand, the deficiency of dynamic noise schedules is that it is reliant on fixed assumptions and leaves unanswered whether it can be applied to dynamically changing, realistic recommendation tasks. 
 
Significance: These findings justify the research motivation of this thesis: to apply robustness frameworks not only to static natural noise, but also to dynamic and changing noise. The short-run experiments support both strengths and limitations of DCCF and hence confirm that not only is the proposed direction the development of adaptive strategies to dynamic noise, but it is also necessary to undertake it. The positive signal of the warm-up results is that we can actually gain some stability, and the larger experiments with multiple datasets and types of noises will become feasible in the final thesis.

  
VI. CONCLUSIONS
In this interim report, the strengths of recommenders have been considered in the case where no access to the noisy user interaction data, i.e., the Disentangled Contrastive Collaborative Filtering (DCCF) model, is available. To motivate the work, it was also understood that existing strong recommendation techniques assume that the noise is fixed, and in practice user behaviour provides dynamic changing noise.
 
Through the literature review we found that some of the most used recommendation techniques, such as collaborative filtering, deep learning, and graph-based, have steadily gotten more accurate over the years, but a major limitation is that they are not very robust. Recent developments of contrastive learning and DCCF demonstrated the possibility to deal with natural noise, although various critical issues are present, such as susceptibility to adversarial attacks, high cost, dependency on fixed noise conditions, and the fact that the training process is highly unstable in the first training stages.
 
DCCF was tested in non-dynamical noise regime and dynamic noise regime using the experimental method in this project and both benchmark data sets and noise injection techniques were used to test this method. Preliminary tests confirm that DCCF works extremely well in clean and constant noisy environment, which explains the importance of intent disentanglement and adaptive augmentation. But also it is seen that performance is poorer when noise is dynamically varying with time, as is the case with ramp-up, burst, and shift conditions. It was discovered that warm-up introduction, at a slow pace, would help decrease instability at the initial stage, a lightweight, but an effective, training stability modification.
 
In general, the initial results indicate the necessity of replacing fixed robustness with approaches capable of responding to changing behaviour of users and noise distribution. The results do confirm the fact that DCCF does provide a healthy background of skilled recommendation but it also reveal the obvious flaws that drive the second section of the research. The project will also be used to design the recommender system to solve the problem of dynamic noise, besides being incorporated in the creation of mechanisms that can be regarded as adaptive to be true, efficient and more trustworthy and reliable in its working in the real world.







x



x

